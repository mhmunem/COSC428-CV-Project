# !pip install tensorflow
# %tensorflow_version 2

import os
# !cd "data/"

# cwd1 = os.getcwd()
# print({cwd1})
# os.chdir(f"{cwd1}/data/")

# !wget http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat
# !wget http://www.ehu.eus/ccwintco/uploads/f/f1/Salinas.mat
# !wget http://www.ehu.eus/ccwintco/uploads/a/a3/Salinas_corrected.mat
# !wget http://www.ehu.eus/ccwintco/uploads/f/fa/Salinas_gt.mat
# #!wget http://www.ehu.eus/ccwintco/uploads/5/50/PaviaU_gt.mat
# # #!wget http://www.ehu.eus/ccwintco/uploads/e/ee/PaviaU.mat
# !wget http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat
# # !wget http://www.ehu.eus/ccwintco/uploads/2/22/Indian_pines.mat
# # %cd ..
# # !pip install spectral
# # !pip install PyWavelets
# # !pip install np_utils
# # !pip install easyspc

# !cd ..

# os.chdir(f"{cwd1}")
# cwd = os.getcwd()
# print({cwd})


import keras
import tensorflow as tf
from tensorflow.keras.layers import Conv2D,Conv2DTranspose, MaxPooling2D, MaxPooling3D, Conv3D, Flatten, Dense, Reshape, BatchNormalization, Layer
from keras.layers import Dropout, Input, SeparableConv2D, LeakyReLU, GlobalAveragePooling2D,AveragePooling2D, MaxPooling2D, LeakyReLU, Add, Activation
from keras.regularizers import l2,l1
from tensorflow.keras.models import Model
from keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.decomposition import PCA
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score
from operator import truediv
# from keras.utils import np_utils
# from plotly.offline import init_notebook_mode
import random
import numpy as np
import matplotlib.pyplot as plt
import scipy.io as sio
import scipy
import os
import spectral
# from easyspc import SPC

# init_notebook_mode(connected=True)
# %matplotlib inline


## GLOBAL VARIABLES
dataset = "IP"
test_ratio = 0.80
windowSize = 11

def oversampleWeakClasses(X, y):
    uniqueLabels, labelCounts = np.unique(y, return_counts=True)
    maxCount = np.max(labelCounts)
    labelInverseRatios = maxCount / labelCounts
    # repeat for every label and concat
    newX = X[y == uniqueLabels[0], :, :, :].repeat(round(labelInverseRatios[0]), axis=0)
    newY = y[y == uniqueLabels[0]].repeat(round(labelInverseRatios[0]), axis=0)
    for label, labelInverseRatio in zip(uniqueLabels[1:], labelInverseRatios[1:]):
        cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)
        cY = y[y == label].repeat(round(labelInverseRatio), axis=0)
        newX = np.concatenate((newX, cX))
        newY = np.concatenate((newY, cY))
    np.random.seed(seed=42)
    rand_perm = np.random.permutation(newY.shape[0])
    newX = newX[rand_perm, :, :, :]
    newY = newY[rand_perm]
    return newX, newY

def loadData(name):
    data_path = os.path.join(os.getcwd(),'data')
    if name == 'IP':
        data = sio.loadmat(os.path.join(data_path, 'Indian_pines_corrected.mat'))['indian_pines_corrected']
        labels = sio.loadmat(os.path.join(data_path, 'Indian_pines_gt.mat'))['indian_pines_gt']
    elif name == 'SA':
        data = sio.loadmat(os.path.join(data_path, 'Salinas_corrected.mat'))['salinas_corrected']
        labels = sio.loadmat(os.path.join(data_path, 'Salinas_gt.mat'))['salinas_gt']
    elif name == 'PU':
        data = sio.loadmat(os.path.join(data_path, 'PaviaU.mat'))['paviaU']
        labels = sio.loadmat(os.path.join(data_path, 'PaviaU_gt.mat'))['paviaU_gt']
    return data, labels
def splitTrainTestSet(X, y, testRatio, randomState=345):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState,
                                                        stratify=y)
    return X_train, X_test, y_train, y_test

def applyPCA(X, numComponents=75):
    newX = np.reshape(X, (-1, X.shape[2]))
    pca = PCA(n_components=numComponents,whiten=True)
    newX = pca.fit_transform(newX)
    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))
    return newX, pca

def padWithZeros(X, margin=2):
    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))
    x_offset = margin
    y_offset = margin
    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X
    return newX

def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):
    margin = int((windowSize - 1) / 2)
    zeroPaddedX = padWithZeros(X, margin=margin)
    # split patches
    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))
    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))
    patchIndex = 0
    for r in range(margin, zeroPaddedX.shape[0] - margin):
        for c in range(margin, zeroPaddedX.shape[1] - margin):
            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]
            patchesData[patchIndex, :, :, :] = patch
            patchesLabels[patchIndex] = y[r-margin, c-margin]
            patchIndex = patchIndex + 1
    if removeZeroLabels:
        patchesData = patchesData[patchesLabels>0,:,:,:]
        patchesLabels = patchesLabels[patchesLabels>0]
        patchesLabels -= 1
    return patchesData, patchesLabels


X, y = loadData(dataset)
print(X.shape)
K = 100 if dataset == 'IP' else 40
print(X.shape)
X,pca = applyPCA(X,numComponents=K)
print(X.shape)
K = X.shape[2]
print(X.shape)

X, y = createImageCubes(X, y, windowSize=windowSize)
print(X.shape)
Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X, y, test_ratio)
print(Xtrain.shape)


Xtrain, ytrain = oversampleWeakClasses(Xtrain, ytrain)
print(Xtrain.shape)
Xtrain = Xtrain.reshape(-1, windowSize, windowSize, K, 1)
print(Xtrain.shape)
ytrain = to_categorical(ytrain)
S = windowSize
L = K
output_units = 9 if (dataset == 'PU' or dataset == 'PC') else 16


from tensorflow.keras.layers import Conv2D, BatchNormalization, LeakyReLU, MaxPooling2D, MultiHeadAttention, LayerNormalization

def sandwich_block(x, filters, kernel_size=(3, 3), pool_size=(2, 2), use_pooling=True, dropout_rate=0.0):
    x = Conv2D(filters=filters, kernel_size=kernel_size, padding='same')(x)
    x = BatchNormalization()(x)
    x = LeakyReLU()(x)
    if use_pooling:
        x = MaxPooling2D(pool_size=pool_size)(x)
    if dropout_rate > 0.0:
        x = Dropout(dropout_rate)(x)
    return x



input_layer = Input((S, S, L, 1))


x = Conv3D(filters=32, kernel_size=(3, 3, 7), activation='relu')(input_layer)
x = Conv3D(filters=16, kernel_size=(3, 3, 5), activation='relu')(x)
x = Conv3D(filters=8, kernel_size=(3, 3, 5), activation='relu')(x)

x_shape = x.shape
x = Reshape((x_shape[1], x_shape[2], x_shape[3]*x_shape[4]))(x)


# Sandwich blocks in place of Conv2D layers
x = sandwich_block(x, filters=64, kernel_size=(3, 3), pool_size=(2, 2), use_pooling=False)

# Self-attention layer between the sandwich blocks
attn_output = MultiHeadAttention(num_heads=8, key_dim=64)(x, x)
attn_output = LayerNormalization(epsilon=1e-6)(attn_output + x)

x = sandwich_block(x, filters=32, kernel_size=(3, 3), pool_size=(2, 2), use_pooling=True)

#

# x = AveragePooling2D(pool_size=(2, 2))(x)
x = Dropout(0.4)(x)
x = GlobalAveragePooling2D()(x)

x = Flatten()(x)

x = Dense(units=256, activation='relu')(x)
x = LeakyReLU()(x)
x = Dropout(0.4)(x)


x = Dense(units=128, activation='relu')(x)
x = LeakyReLU()(x)

output_layer = Dense(units=output_units, activation='softmax')(x)


model = Model(inputs=input_layer, outputs=output_layer)
adam = Adam(learning_rate=0.001)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])
filepath = f"best-model-{dataset}.keras"
checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]


history = model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=10, callbacks=callbacks_list)


from tensorflow.keras.utils import to_categorical
model.load_weights(f"best-model-{dataset}.keras")
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
Xtest = Xtest.reshape(-1, windowSize, windowSize, K, 1)
ytest = to_categorical(ytest)
Y_pred_test = model.predict(Xtest)
y_pred_test = np.argmax(Y_pred_test, axis=1)
classification = classification_report(np.argmax(ytest, axis=1), y_pred_test)
print(classification)


def AA_andEachClassAccuracy(confusion_matrix):
    counter = confusion_matrix.shape[0]
    list_diag = np.diag(confusion_matrix)
    list_raw_sum = np.sum(confusion_matrix, axis=1)
    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))
    average_acc = np.mean(each_acc)
    return each_acc, average_acc

def reports (X_test,y_test,name):
    Y_pred = model.predict(X_test)
    y_pred = np.argmax(Y_pred, axis=1)
    if name == 'IP':
        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'
                        ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed',
                        'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',
                        'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',
                        'Stone-Steel-Towers']
    elif name == 'SA':
        target_names = ['Brocoli_green_weeds_1','Brocoli_green_weeds_2','Fallow','Fallow_rough_plow','Fallow_smooth',
                        'Stubble','Celery','Grapes_untrained','Soil_vinyard_develop','Corn_senesced_green_weeds',
                        'Lettuce_romaine_4wk','Lettuce_romaine_5wk','Lettuce_romaine_6wk','Lettuce_romaine_7wk',
                        'Vinyard_untrained','Vinyard_vertical_trellis']
    elif name == 'PU':
        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted metal sheets','Bare Soil','Bitumen',
                        'Self-Blocking Bricks','Shadows']

    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)
    oa = accuracy_score(np.argmax(y_test, axis=1), y_pred)
    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)
    each_acc, aa = AA_andEachClassAccuracy(confusion)
    kappa = cohen_kappa_score(np.argmax(y_test, axis=1), y_pred)
    score = model.evaluate(X_test, y_test, batch_size=32)
    Test_Loss =  score[0]*100
    Test_accuracy = score[1]*100

    return classification, confusion, Test_Loss, Test_accuracy, oa*100, each_acc*100, aa*100, kappa*100



classification, confusion, Test_loss, Test_accuracy, oa, each_acc, aa, kappa = reports(Xtest,ytest,dataset)
print(each_acc)
classification = str(classification)
confusion = str(confusion)
file_name = f"classification_report-{dataset}.txt"

with open(file_name, 'w') as x_file:
    x_file.write('{} Test loss (%)'.format(Test_loss))
    x_file.write('\n')
    x_file.write('{} Test accuracy (%)'.format(Test_accuracy))
    x_file.write('\n')
    x_file.write('\n')
    x_file.write('{} Kappa accuracy (%)'.format(kappa))
    x_file.write('\n')
    x_file.write('{} Overall accuracy (%)'.format(oa))
    x_file.write('\n')
    x_file.write('{} Average accuracy (%)'.format(aa))
    x_file.write('\n')
    x_file.write('\n')
    x_file.write('{}'.format(classification))
    x_file.write('\n')
    x_file.write('{}'.format(confusion))



!cat classification_report.txt


def Patch(data,height_index,width_index):
    height_slice = slice(height_index, height_index+PATCH_SIZE)
    width_slice = slice(width_index, width_index+PATCH_SIZE)
    patch = data[height_slice, width_slice, :]
    return patch

X, y = loadData(dataset)
height = y.shape[0]
width = y.shape[1]
PATCH_SIZE = windowSize
numComponents = K
X,pca = applyPCA(X, numComponents=numComponents)
X = padWithZeros(X, PATCH_SIZE//2)
outputs = np.zeros((height,width))
for i in range(height):
    for j in range(width):
        target = int(y[i,j])
        if target == 0 :
            continue
        else :
            image_patch=Patch(X,i,j)
            X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2], 1).astype('float32')
            prediction = (model.predict(X_test_image))
            prediction = np.argmax(prediction, axis=1)
            outputs[i][j] = prediction+1


ground_truth = spectral.imshow(classes = y,figsize =(10,10))


predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(10,10))


# from google.colab import files
name = f"image_{dataset}.pdf"
spectral.save_rgb(name, outputs.astype(int), colors=spectral.spy_colors)
# files.download(name)



